{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from preprocess import resize_input, train_test_split, read_raw\n",
    "from ear_dataset import EarDataset\n",
    "\n",
    "import cv2\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(os.listdir(\"../UERC/\"))\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.arange(1, n_classes + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(910, 400)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly choose 30% of the classes\n",
    "test_mask = np.random.choice([True, False], size=n_classes, p=[0.3, 0.7])\n",
    "\n",
    "test = classes[test_mask]\n",
    "train = classes[~test_mask]\n",
    "\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_subjects = np.loadtxt(\"../train_subjects_mask.txt\")\n",
    "# test_subjects = np.loadtxt(\"../test_subjects_mask.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects = train\n",
    "test_subjects = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_subjects_mask.txt\", train)\n",
    "np.savetxt(\"test_subjects_mask.txt\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_path = \"../UERC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_data = os.listdir(data_path)\n",
    "\n",
    "ear_imgs = {}\n",
    "for person in ear_data:\n",
    "    if int(person) not in train_subjects:\n",
    "        continue\n",
    "\n",
    "    imgs = os.listdir(\"%s/%s\" % (data_path, person))\n",
    "    try:\n",
    "        ear_imgs[person] = [\n",
    "            cv2.cvtColor(\n",
    "                np.asarray(Image.open(f\"{data_path}/{person}/{img}\")), cv2.COLOR_BGR2RGB\n",
    "            )\n",
    "            for img in imgs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(ear_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120809, 120809, 52270, 52270)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train), len(X_eval), len(y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120809, 52270)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(910, 910)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train)), len(set(y_eval))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120809, 120809)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = resize_input(X_train, tgt_size=64, mode=\"train\")\n",
    "\n",
    "train_dataset = EarDataset(X_train, y_train)\n",
    "\n",
    "len(train_dataset.data), len(train_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0001', '0002', '0004', '0005', '0006', '0007', '0008', '0009', '0011', '0013', '0014', '0015', '0016', '0017', '0019', '0020', '0022', '0023', '0024', '0025', '0026', '0030', '0031', '0032', '0033', '0034', '0035', '0037', '0038', '0040', '0042', '0043', '0044', '0045', '0046', '0048', '0049', '0050', '0052', '0053', '0054', '0056', '0057', '0058', '0059', '0060', '0061', '0062', '0063', '0064', '0065', '0066', '0068', '0069', '0070', '0071', '0074', '0075', '0078', '0080', '0082', '0083', '0084', '0085', '0086', '0088', '0090', '0091', '0092', '0094', '0095', '0096', '0098', '0101', '0103', '0104', '0105', '0106', '0107', '0109', '0110', '0111', '0112', '0113', '0114', '0115', '0116', '0117', '0118', '0121', '0122', '0123', '0124', '0126', '0127', '0128', '0129', '0130', '0131', '0133', '0134', '0135', '0136', '0137', '0138', '0140', '0141', '0142', '0143', '0144', '0145', '0146', '0147', '0150', '0151', '0152', '0154', '0155', '0157', '0158', '0164', '0165', '0167', '0168', '0169', '0170', '0171', '0172', '0173', '0174', '0176', '0177', '0178', '0179', '0180', '0182', '0183', '0187', '0188', '0189', '0191', '0192', '0193', '0196', '0198', '0199', '0203', '0204', '0205', '0207', '0208', '0209', '0210', '0211', '0212', '0213', '0214', '0215', '0216', '0217', '0218', '0222', '0223', '0224', '0225', '0226', '0227', '0228', '0229', '0231', '0232', '0233', '0234', '0235', '0236', '0237', '0240', '0241', '0242', '0244', '0245', '0246', '0247', '0248', '0249', '0251', '0252', '0253', '0254', '0255', '0256', '0257', '0260', '0261', '0262', '0263', '0264', '0265', '0266', '0267', '0268', '0269', '0270', '0271', '0272', '0274', '0275', '0276', '0278', '0279', '0280', '0282', '0283', '0285', '0286', '0287', '0288', '0292', '0293', '0295', '0296', '0299', '0300', '0301', '0303', '0304', '0305', '0306', '0307', '0308', '0309', '0310', '0311', '0314', '0316', '0319', '0320', '0321', '0322', '0323', '0324', '0326', '0327', '0328', '0329', '0330', '0331', '0332', '0333', '0334', '0339', '0340', '0341', '0342', '0343', '0344', '0345', '0346', '0349', '0350', '0352', '0354', '0356', '0359', '0360', '0363', '0365', '0366', '0368', '0369', '0370', '0373', '0374', '0375', '0377', '0380', '0381', '0382', '0383', '0387', '0391', '0393', '0395', '0396', '0399', '0401', '0404', '0405', '0406', '0407', '0408', '0409', '0410', '0411', '0412', '0415', '0419', '0420', '0421', '0422', '0423', '0425', '0426', '0427', '0428', '0429', '0430', '0431', '0432', '0433', '0434', '0435', '0436', '0438', '0441', '0442', '0444', '0445', '0447', '0452', '0453', '0455', '0456', '0457', '0458', '0459', '0461', '0462', '0463', '0465', '0466', '0468', '0469', '0470', '0471', '0472', '0473', '0476', '0477', '0478', '0479', '0480', '0481', '0484', '0486', '0487', '0488', '0489', '0490', '0492', '0493', '0494', '0495', '0499', '0500', '0501', '0504', '0505', '0509', '0513', '0515', '0517', '0518', '0519', '0520', '0524', '0525', '0526', '0527', '0529', '0531', '0532', '0533', '0534', '0536', '0537', '0539', '0540', '0541', '0542', '0543', '0546', '0548', '0551', '0553', '0554', '0555', '0556', '0557', '0558', '0560', '0561', '0562', '0563', '0564', '0565', '0568', '0569', '0570', '0571', '0573', '0574', '0576', '0577', '0578', '0580', '0581', '0582', '0583', '0585', '0588', '0589', '0590', '0591', '0592', '0593', '0595', '0596', '0597', '0598', '0599', '0602', '0603', '0604', '0606', '0607', '0610', '0611', '0612', '0613', '0614', '0615', '0617', '0619', '0620', '0621', '0622', '0626', '0630', '0632', '0637', '0638', '0639', '0643', '0644', '0645', '0646', '0647', '0648', '0651', '0653', '0654', '0655', '0657', '0659', '0660', '0661', '0663', '0665', '0666', '0667', '0668', '0669', '0670', '0675', '0676', '0679', '0680', '0681', '0682', '0683', '0684', '0685', '0686', '0688', '0690', '0691', '0692', '0695', '0696', '0699', '0702', '0703', '0705', '0706', '0708', '0709', '0710', '0713', '0715', '0716', '0717', '0718', '0719', '0721', '0723', '0727', '0728', '0729', '0730', '0731', '0732', '0733', '0734', '0735', '0736', '0737', '0738', '0739', '0742', '0743', '0744', '0745', '0746', '0749', '0750', '0752', '0756', '0757', '0758', '0761', '0762', '0763', '0764', '0765', '0766', '0767', '0768', '0769', '0770', '0771', '0772', '0773', '0774', '0775', '0776', '0777', '0778', '0780', '0781', '0782', '0783', '0785', '0786', '0787', '0788', '0791', '0792', '0793', '0795', '0796', '0798', '0799', '0800', '0803', '0804', '0805', '0806', '0807', '0808', '0809', '0811', '0815', '0816', '0817', '0820', '0822', '0823', '0824', '0825', '0826', '0828', '0830', '0831', '0833', '0834', '0835', '0837', '0840', '0841', '0842', '0845', '0846', '0848', '0849', '0850', '0851', '0852', '0853', '0855', '0856', '0858', '0862', '0863', '0865', '0867', '0868', '0870', '0871', '0872', '0873', '0874', '0875', '0876', '0877', '0878', '0879', '0881', '0883', '0885', '0886', '0887', '0888', '0889', '0890', '0896', '0897', '0898', '0899', '0900', '0901', '0902', '0905', '0907', '0910', '0913', '0915', '0916', '0918', '0919', '0920', '0922', '0924', '0925', '0927', '0928', '0929', '0931', '0932', '0933', '0934', '0936', '0937', '0938', '0939', '0940', '0941', '0944', '0945', '0946', '0947', '0948', '0950', '0951', '0952', '0953', '0954', '0957', '0958', '0959', '0961', '0962', '0963', '0964', '0965', '0966', '0969', '0970', '0971', '0972', '0974', '0977', '0978', '0980', '0981', '0982', '0983', '0984', '0988', '0989', '0990', '0991', '0992', '0994', '0995', '0996', '0997', '0998', '0999', '1000', '1001', '1002', '1003', '1004', '1006', '1007', '1008', '1010', '1011', '1012', '1013', '1014', '1015', '1017', '1019', '1021', '1023', '1024', '1025', '1026', '1027', '1029', '1030', '1031', '1032', '1033', '1035', '1038', '1039', '1040', '1042', '1043', '1044', '1047', '1048', '1050', '1051', '1052', '1053', '1054', '1056', '1058', '1060', '1062', '1063', '1064', '1065', '1066', '1067', '1069', '1070', '1071', '1074', '1075', '1078', '1079', '1081', '1082', '1083', '1084', '1086', '1087', '1089', '1091', '1098', '1099', '1100', '1101', '1102', '1103', '1104', '1105', '1107', '1108', '1109', '1110', '1111', '1113', '1114', '1115', '1117', '1118', '1119', '1120', '1121', '1123', '1124', '1125', '1126', '1128', '1130', '1133', '1134', '1135', '1136', '1139', '1140', '1143', '1144', '1146', '1147', '1148', '1149', '1150', '1152', '1154', '1155', '1156', '1157', '1159', '1160', '1161', '1162', '1163', '1164', '1165', '1166', '1167', '1168', '1170', '1171', '1172', '1173', '1174', '1175', '1176', '1177', '1178', '1182', '1183', '1187', '1190', '1192', '1193', '1194', '1197', '1198', '1199', '1200', '1204', '1205', '1207', '1208', '1210', '1212', '1214', '1215', '1217', '1218', '1221', '1223', '1224', '1227', '1228', '1229', '1230', '1231', '1232', '1233', '1234', '1235', '1236', '1237', '1239', '1240', '1241', '1242', '1243', '1244', '1245', '1246', '1247', '1249', '1250', '1252', '1253', '1255', '1256', '1258', '1260', '1261', '1262', '1264', '1265', '1266', '1267', '1268', '1269', '1270', '1271', '1272', '1273', '1274', '1275', '1276', '1277', '1278', '1279', '1280', '1281', '1284', '1285', '1286', '1287', '1288', '1289', '1290', '1291', '1292', '1293', '1294', '1295', '1297', '1299', '1300', '1304', '1306', '1308', '1309', '1310'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ear_imgs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, \"data/train_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = resize_input(X_eval, tgt_size=64, mode=\"test\")\n",
    "\n",
    "eval_dataset = EarDataset(X_eval, y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52270, 52270)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataset.data), len(eval_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(eval_dataset, \"data/eval_dataset.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
