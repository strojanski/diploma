{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triplet_dataset import PairDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torch.load(\"data/train_dataset.pt\")\n",
    "eval_ds = torch.load(\"data/eval_dataset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.reshuffle()\n",
    "# eval_ds.reshuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52270, 52270, 52270)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_ds.data), len(eval_ds.labels), len(eval_ds.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set_size = 30_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds.data = eval_ds.data[:eval_set_size]\n",
    "eval_ds.labels = eval_ds.labels[:eval_set_size]\n",
    "eval_ds.pairs = eval_ds.pairs[:eval_set_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_majority(ds, get_value_counts=False):\n",
    "    \"\"\"Returns accuracy of the majority classifier\"\"\"\n",
    "    # Get the labels from the dataset\n",
    "    labels = [ys[0] for Xs, ys in ds.pairs]\n",
    "    \n",
    "    # Calculate the majority class label and its count\n",
    "    majority_class = np.argmax(np.bincount(labels))\n",
    "    majority_class_count = np.sum(labels == majority_class)\n",
    "    \n",
    "    # Calculate the accuracy of the majority classifier\n",
    "    accuracy = majority_class_count / len(labels)\n",
    "    \n",
    "    if get_value_counts:\n",
    "        return majority_class, accuracy, np.bincount(labels)\n",
    "    \n",
    "    return majority_class, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1028, 0.003733165575412428)\n"
     ]
    }
   ],
   "source": [
    "print(get_majority(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_dl = DataLoader(eval_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(a, b), (c, d) = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_embeddings_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize_batch(batch):\n",
    "    min_vals = np.min(batch, axis=1, keepdims=True)\n",
    "    max_vals = np.max(batch, axis=1, keepdims=True)\n",
    "    normalized_batch = (batch - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, valid_loader, epochs, save_path=\"models/model\"):\n",
    "    a = True\n",
    "    best_val_loss = float('inf')  # Initialize with infinity\n",
    "    \n",
    "    model = model.to(device)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            input_1, input_2 = inputs\n",
    "            label_1, label_2 = labels\n",
    "            \n",
    "            input_1 = input_1.to(device)\n",
    "            input_2 = input_2.to(device)\n",
    "            \n",
    "            emb_1 = model(input_1)\n",
    "            emb_2 = model(input_2)          \n",
    "            \n",
    "            y = torch.tensor(np.array([1 if l1 == l2 else -1 for l1, l2 in zip(label_1, label_2)])).to(device)\n",
    "            # print(label_1, label_2, y.cpu())\n",
    "\n",
    "            loss = criterion(emb_1, emb_2, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        print(\"Train loss:\", train_loss)\n",
    "        loss_history.append(loss.item())    \n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        embeddings = []\n",
    "        targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                input_1, input_2 = inputs\n",
    "                label_1, label_2 = labels\n",
    "                \n",
    "                batch_emb, batch_lab = [], []\n",
    "                \n",
    "                input_1 = input_1.to(device)\n",
    "                input_2 = input_2.to(device)\n",
    "                \n",
    "                emb_1 = model(input_1)\n",
    "                emb_2 = model(input_2)\n",
    "                \n",
    "                # max_1 = torch.max(emb_1)\n",
    "                # min_1 = torch.min(emb_1)\n",
    "                # max_2 = torch.max(emb_2)\n",
    "                # min_2 = torch.min(emb_2)\n",
    "\n",
    "\n",
    "                # emb_1 = (emb_1 - min_1) / (max_1 - min_1)\n",
    "                # emb_2 = (emb_2 - min_2) / (max_2 - min_2)\n",
    "                \n",
    "                # embeddings.extend(emb_anchor)\n",
    "                batch_emb.extend(emb_1.cpu())\n",
    "                batch_emb.extend(emb_2.cpu())\n",
    "                \n",
    "                # labels.extend(label_anchor)\n",
    "                batch_lab.extend(label_1.cpu().tolist())\n",
    "                batch_lab.extend(label_2.cpu().tolist())\n",
    "                \n",
    "                embeddings.extend(batch_emb)\n",
    "                targets.extend(batch_lab)\n",
    "                \n",
    "                y = torch.tensor(np.array([1 if l1 == l2 else -1 for l1, l2 in zip(label_1, label_2)])).to(device)\n",
    "                \n",
    "                loss = criterion(emb_1, emb_2, y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(valid_loader)\n",
    "        \n",
    "        for i in range(len(embeddings)):\n",
    "            embeddings[i] = embeddings[i].detach().numpy()\n",
    "            \n",
    "        embeddings = np.array(embeddings)        \n",
    "        targets = np.array(targets)\n",
    "        \n",
    "        # Min max normalize embeddings\n",
    "        embeddings = min_max_normalize_batch(embeddings)\n",
    "        \n",
    "        # Get cosine similarity for all embeddings\n",
    "        similarity_matrix = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Fill diagonal with inf\n",
    "        np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "        \n",
    "        # Find the index of max sim for each embedding\n",
    "        most_similar_indices = np.argmax(similarity_matrix, axis=1)\n",
    "        \n",
    "        # Extract the labels of the most similar items\n",
    "        predicted_labels = [targets[i] for i in most_similar_indices]\n",
    "        \n",
    "        \n",
    "        # Get number of correct predictions\n",
    "        # n_correct = sum(1 for true_label, predicted_label in zip(labels, predicted_labels) if true_label == predicted_label)\n",
    "        n_correct = sum(1 for true, pred in zip(targets, predicted_labels) if true == pred)\n",
    "        \n",
    "        print(\"Number of correct matches: \", n_correct)\n",
    "\n",
    "        val_acc = 100 * (n_correct / len(embeddings))\n",
    "\n",
    "        accuracy_history.append(val_acc)\n",
    "        \n",
    "        print(f\"Validation accuracy: {val_acc:0.3f}%\")\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if a:\n",
    "            special_embeddings_batch.append(embeddings)\n",
    "            a = False\n",
    "        \n",
    "        # Save the model if validation loss is improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{save_path}_{epoch+1}\")\n",
    "            print(\"Model saved with validation loss:\", best_val_loss)\n",
    "            print()\n",
    "        \n",
    "        embeddings = []\n",
    "        similarity_matrix = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get model & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CosineEmbeddingLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESNET_VERSION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special_embeddings_batch = min_max_normalize_batch(special_embeddings_batch)\n",
    "np.mean(special_embeddings_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# baseline = models.resnet50(pretrained=True)\n",
    "    \n",
    "# baseline.fc = nn.Linear(2048, 910)\n",
    "    \n",
    "# if os.path.exists(f\"models/baseline_resnet50.pt.pt\"):\n",
    "#     baseline.load_state_dict(torch.load(f\"models/baseline_resnet50.pt\"))\n",
    "\n",
    "# baseline.fc = torch.nn.Identity()\n",
    "# model = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if RESNET_VERSION == 50:\n",
    "    model = models.resnet50(pretrained=True)\n",
    "else:\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    \n",
    "model.fc = torch.nn.Identity()\n",
    "\n",
    "if os.path.exists(f\"models/resnet{RESNET_VERSION}.pt\"):\n",
    "    model.load_state_dict(torch.load(f\"models/resnet{RESNET_VERSION}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import SupConLoss\n",
    "LR = 1e-2\n",
    "\n",
    "n_epochs = 2000\n",
    "\n",
    "# TODO: Fix margin\n",
    "criterion = CosineEmbeddingLoss(margin=-1) #torch.nn.TripletMarginLoss(margin=1000, p=2) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 441.0721831917763\n",
      "Number of correct matches:  12267\n",
      "Validation accuracy: 20.445%\n",
      "Epoch 1/2000, Train Loss: 0.9345, Valid Loss: 0.9347\n",
      "Model saved with validation loss: 0.934712221056728\n",
      "\n",
      "Train loss: 434.7992181777954\n",
      "Number of correct matches:  12556\n",
      "Validation accuracy: 20.927%\n",
      "Epoch 2/2000, Train Loss: 0.9212, Valid Loss: 0.9076\n",
      "Model saved with validation loss: 0.9075748137498306\n",
      "\n",
      "Train loss: 430.40973049402237\n",
      "Number of correct matches:  12604\n",
      "Validation accuracy: 21.007%\n",
      "Epoch 3/2000, Train Loss: 0.9119, Valid Loss: 0.9083\n",
      "Train loss: 429.2395758628845\n",
      "Number of correct matches:  12572\n",
      "Validation accuracy: 20.953%\n",
      "Epoch 4/2000, Train Loss: 0.9094, Valid Loss: 0.9013\n",
      "Model saved with validation loss: 0.9013452075295529\n",
      "\n",
      "Train loss: 427.5305244922638\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Number of correct matches:  12575\n",
      "Validation accuracy: 20.958%\n",
      "Epoch 5/2000, Train Loss: 0.9058, Valid Loss: 0.9032\n",
      "Train loss: 424.1727052330971\n",
      "Number of correct matches:  12567\n",
      "Validation accuracy: 20.945%\n",
      "Epoch 6/2000, Train Loss: 0.8987, Valid Loss: 0.8959\n",
      "Model saved with validation loss: 0.8958533951791666\n",
      "\n",
      "Train loss: 422.8543511033058\n",
      "Number of correct matches:  12605\n",
      "Validation accuracy: 21.008%\n",
      "Epoch 7/2000, Train Loss: 0.8959, Valid Loss: 0.8931\n",
      "Model saved with validation loss: 0.8931427587897091\n",
      "\n",
      "Train loss: 421.94723081588745\n",
      "Number of correct matches:  12691\n",
      "Validation accuracy: 21.152%\n",
      "Epoch 8/2000, Train Loss: 0.8940, Valid Loss: 0.8908\n",
      "Model saved with validation loss: 0.8908496559676478\n",
      "\n",
      "Train loss: 421.1745203733444\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Number of correct matches:  12617\n",
      "Validation accuracy: 21.028%\n",
      "Epoch 9/2000, Train Loss: 0.8923, Valid Loss: 0.8917\n",
      "Train loss: 420.29066228866577\n",
      "Number of correct matches:  12641\n",
      "Validation accuracy: 21.068%\n",
      "Epoch 10/2000, Train Loss: 0.8904, Valid Loss: 0.8910\n",
      "Train loss: 419.8103942871094\n",
      "Number of correct matches:  12609\n",
      "Validation accuracy: 21.015%\n",
      "Epoch 11/2000, Train Loss: 0.8894, Valid Loss: 0.8911\n",
      "Train loss: 419.6139962673187\n",
      "Number of correct matches:  12471\n",
      "Validation accuracy: 20.785%\n",
      "Epoch 12/2000, Train Loss: 0.8890, Valid Loss: 0.8908\n",
      "Model saved with validation loss: 0.8907818925582757\n",
      "\n",
      "Train loss: 419.6998829841614\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Number of correct matches:  12582\n",
      "Validation accuracy: 20.970%\n",
      "Epoch 13/2000, Train Loss: 0.8892, Valid Loss: 0.8918\n",
      "Train loss: 419.4868402481079\n",
      "Number of correct matches:  12531\n",
      "Validation accuracy: 20.885%\n",
      "Epoch 14/2000, Train Loss: 0.8887, Valid Loss: 0.8910\n",
      "Train loss: 419.3863806128502\n",
      "Number of correct matches:  12613\n",
      "Validation accuracy: 21.022%\n",
      "Epoch 15/2000, Train Loss: 0.8885, Valid Loss: 0.8911\n",
      "Train loss: 419.5337556004524\n",
      "Number of correct matches:  12631\n",
      "Validation accuracy: 21.052%\n",
      "Epoch 16/2000, Train Loss: 0.8888, Valid Loss: 0.8906\n",
      "Model saved with validation loss: 0.8906099119428861\n",
      "\n",
      "Train loss: 419.45658779144287\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Number of correct matches:  12643\n",
      "Validation accuracy: 21.072%\n",
      "Epoch 17/2000, Train Loss: 0.8887, Valid Loss: 0.8902\n",
      "Model saved with validation loss: 0.8901521033149654\n",
      "\n",
      "Train loss: 419.3792130947113\n",
      "Number of correct matches:  12582\n",
      "Validation accuracy: 20.970%\n",
      "Epoch 18/2000, Train Loss: 0.8885, Valid Loss: 0.8901\n",
      "Model saved with validation loss: 0.890094019093756\n",
      "\n",
      "Train loss: 419.5284386277199\n",
      "Number of correct matches:  12639\n",
      "Validation accuracy: 21.065%\n",
      "Epoch 19/2000, Train Loss: 0.8888, Valid Loss: 0.8917\n",
      "Train loss: 419.58857518434525\n",
      "Number of correct matches:  12548\n",
      "Validation accuracy: 20.913%\n",
      "Epoch 20/2000, Train Loss: 0.8890, Valid Loss: 0.8911\n",
      "Train loss: 419.67472064495087\n",
      "Epoch 00021: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Number of correct matches:  12629\n",
      "Validation accuracy: 21.048%\n",
      "Epoch 21/2000, Train Loss: 0.8891, Valid Loss: 0.8921\n",
      "Train loss: 419.6682345867157\n",
      "Number of correct matches:  12585\n",
      "Validation accuracy: 20.975%\n",
      "Epoch 22/2000, Train Loss: 0.8891, Valid Loss: 0.8906\n",
      "Train loss: 419.4088568687439\n",
      "Number of correct matches:  12568\n",
      "Validation accuracy: 20.947%\n",
      "Epoch 23/2000, Train Loss: 0.8886, Valid Loss: 0.8908\n",
      "Train loss: 419.7194795012474\n",
      "Number of correct matches:  12570\n",
      "Validation accuracy: 20.950%\n",
      "Epoch 24/2000, Train Loss: 0.8892, Valid Loss: 0.8929\n",
      "Train loss: 419.5442333817482\n",
      "Epoch 00025: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Number of correct matches:  12630\n",
      "Validation accuracy: 21.050%\n",
      "Epoch 25/2000, Train Loss: 0.8889, Valid Loss: 0.8918\n",
      "Train loss: 419.3604836463928\n",
      "Number of correct matches:  12588\n",
      "Validation accuracy: 20.980%\n",
      "Epoch 26/2000, Train Loss: 0.8885, Valid Loss: 0.8919\n",
      "Train loss: 419.52075469493866\n",
      "Number of correct matches:  12561\n",
      "Validation accuracy: 20.935%\n",
      "Epoch 27/2000, Train Loss: 0.8888, Valid Loss: 0.8910\n",
      "Train loss: 419.5617235302925\n",
      "Number of correct matches:  12613\n",
      "Validation accuracy: 21.022%\n",
      "Epoch 28/2000, Train Loss: 0.8889, Valid Loss: 0.8911\n",
      "Train loss: 419.34077060222626\n",
      "Number of correct matches:  12578\n",
      "Validation accuracy: 20.963%\n",
      "Epoch 29/2000, Train Loss: 0.8884, Valid Loss: 0.8914\n",
      "Train loss: 419.348994910717\n",
      "Number of correct matches:  12578\n",
      "Validation accuracy: 20.963%\n",
      "Epoch 30/2000, Train Loss: 0.8885, Valid Loss: 0.8907\n",
      "Train loss: 419.4663584828377\n",
      "Number of correct matches:  12573\n",
      "Validation accuracy: 20.955%\n",
      "Epoch 31/2000, Train Loss: 0.8887, Valid Loss: 0.8912\n",
      "Train loss: 419.5633724331856\n",
      "Number of correct matches:  12632\n",
      "Validation accuracy: 21.053%\n",
      "Epoch 32/2000, Train Loss: 0.8889, Valid Loss: 0.8923\n",
      "Train loss: 419.1877675652504\n",
      "Number of correct matches:  12636\n",
      "Validation accuracy: 21.060%\n",
      "Epoch 33/2000, Train Loss: 0.8881, Valid Loss: 0.8909\n",
      "Train loss: 419.467707157135\n",
      "Number of correct matches:  12596\n",
      "Validation accuracy: 20.993%\n",
      "Epoch 34/2000, Train Loss: 0.8887, Valid Loss: 0.8908\n",
      "Train loss: 419.58145022392273\n",
      "Number of correct matches:  12666\n",
      "Validation accuracy: 21.110%\n",
      "Epoch 35/2000, Train Loss: 0.8889, Valid Loss: 0.8915\n",
      "Train loss: 419.53106904029846\n",
      "Number of correct matches:  12756\n",
      "Validation accuracy: 21.260%\n",
      "Epoch 36/2000, Train Loss: 0.8888, Valid Loss: 0.8901\n",
      "Model saved with validation loss: 0.8900505875126791\n",
      "\n",
      "Train loss: 419.5691255927086\n",
      "Number of correct matches:  12605\n",
      "Validation accuracy: 21.008%\n",
      "Epoch 37/2000, Train Loss: 0.8889, Valid Loss: 0.8912\n",
      "Train loss: 419.59250324964523\n",
      "Number of correct matches:  12609\n",
      "Validation accuracy: 21.015%\n",
      "Epoch 38/2000, Train Loss: 0.8890, Valid Loss: 0.8914\n",
      "Train loss: 419.4787822961807\n",
      "Number of correct matches:  12630\n",
      "Validation accuracy: 21.050%\n",
      "Epoch 39/2000, Train Loss: 0.8887, Valid Loss: 0.8902\n",
      "Train loss: 419.56923174858093\n",
      "Number of correct matches:  12682\n",
      "Validation accuracy: 21.137%\n",
      "Epoch 40/2000, Train Loss: 0.8889, Valid Loss: 0.8901\n",
      "Train loss: 419.50784158706665\n",
      "Number of correct matches:  12649\n",
      "Validation accuracy: 21.082%\n",
      "Epoch 41/2000, Train Loss: 0.8888, Valid Loss: 0.8902\n",
      "Train loss: 419.5954969525337\n",
      "Number of correct matches:  12585\n",
      "Validation accuracy: 20.975%\n",
      "Epoch 42/2000, Train Loss: 0.8890, Valid Loss: 0.8902\n",
      "Train loss: 419.31177842617035\n",
      "Number of correct matches:  12545\n",
      "Validation accuracy: 20.908%\n",
      "Epoch 43/2000, Train Loss: 0.8884, Valid Loss: 0.8904\n",
      "Train loss: 419.5387589931488\n",
      "Number of correct matches:  12570\n",
      "Validation accuracy: 20.950%\n",
      "Epoch 44/2000, Train Loss: 0.8889, Valid Loss: 0.8914\n",
      "Train loss: 419.4845303297043\n",
      "Number of correct matches:  12523\n",
      "Validation accuracy: 20.872%\n",
      "Epoch 45/2000, Train Loss: 0.8887, Valid Loss: 0.8917\n",
      "Train loss: 419.53218775987625\n",
      "Number of correct matches:  12597\n",
      "Validation accuracy: 20.995%\n",
      "Epoch 46/2000, Train Loss: 0.8888, Valid Loss: 0.8899\n",
      "Model saved with validation loss: 0.8898536562919617\n",
      "\n",
      "Train loss: 419.5185095667839\n",
      "Number of correct matches:  12589\n",
      "Validation accuracy: 20.982%\n",
      "Epoch 47/2000, Train Loss: 0.8888, Valid Loss: 0.8911\n",
      "Train loss: 419.66152971982956\n",
      "Number of correct matches:  12635\n",
      "Validation accuracy: 21.058%\n",
      "Epoch 48/2000, Train Loss: 0.8891, Valid Loss: 0.8910\n",
      "Train loss: 419.48057919740677\n",
      "Number of correct matches:  12641\n",
      "Validation accuracy: 21.068%\n",
      "Epoch 49/2000, Train Loss: 0.8887, Valid Loss: 0.8904\n",
      "Train loss: 419.37255322933197\n",
      "Number of correct matches:  12556\n",
      "Validation accuracy: 20.927%\n",
      "Epoch 50/2000, Train Loss: 0.8885, Valid Loss: 0.8906\n",
      "Train loss: 419.63666319847107\n",
      "Number of correct matches:  12602\n",
      "Validation accuracy: 21.003%\n",
      "Epoch 51/2000, Train Loss: 0.8891, Valid Loss: 0.8919\n",
      "Train loss: 419.6253955960274\n",
      "Number of correct matches:  12657\n",
      "Validation accuracy: 21.095%\n",
      "Epoch 52/2000, Train Loss: 0.8890, Valid Loss: 0.8905\n",
      "Train loss: 419.49496364593506\n",
      "Number of correct matches:  12613\n",
      "Validation accuracy: 21.022%\n",
      "Epoch 53/2000, Train Loss: 0.8888, Valid Loss: 0.8903\n",
      "Train loss: 419.44854760169983\n",
      "Number of correct matches:  12640\n",
      "Validation accuracy: 21.067%\n",
      "Epoch 54/2000, Train Loss: 0.8887, Valid Loss: 0.8903\n",
      "Train loss: 419.40514755249023\n",
      "Number of correct matches:  12687\n",
      "Validation accuracy: 21.145%\n",
      "Epoch 55/2000, Train Loss: 0.8886, Valid Loss: 0.8912\n",
      "Train loss: 419.3788225054741\n",
      "Number of correct matches:  12534\n",
      "Validation accuracy: 20.890%\n",
      "Epoch 56/2000, Train Loss: 0.8885, Valid Loss: 0.8926\n",
      "Train loss: 419.49280339479446\n",
      "Number of correct matches:  12624\n",
      "Validation accuracy: 21.040%\n",
      "Epoch 57/2000, Train Loss: 0.8888, Valid Loss: 0.8921\n",
      "Train loss: 419.43194913864136\n",
      "Number of correct matches:  12737\n",
      "Validation accuracy: 21.228%\n",
      "Epoch 58/2000, Train Loss: 0.8886, Valid Loss: 0.8911\n",
      "Train loss: 419.2426807284355\n",
      "Number of correct matches:  12723\n",
      "Validation accuracy: 21.205%\n",
      "Epoch 59/2000, Train Loss: 0.8882, Valid Loss: 0.8916\n",
      "Train loss: 419.53238493204117\n",
      "Number of correct matches:  12579\n",
      "Validation accuracy: 20.965%\n",
      "Epoch 60/2000, Train Loss: 0.8888, Valid Loss: 0.8900\n",
      "Train loss: 419.5125480890274\n",
      "Number of correct matches:  12636\n",
      "Validation accuracy: 21.060%\n",
      "Epoch 61/2000, Train Loss: 0.8888, Valid Loss: 0.8912\n",
      "Train loss: 419.23078095912933\n",
      "Number of correct matches:  12593\n",
      "Validation accuracy: 20.988%\n",
      "Epoch 62/2000, Train Loss: 0.8882, Valid Loss: 0.8917\n",
      "Train loss: 419.5310791730881\n",
      "Number of correct matches:  12651\n",
      "Validation accuracy: 21.085%\n",
      "Epoch 63/2000, Train Loss: 0.8888, Valid Loss: 0.8905\n",
      "Train loss: 419.35799592733383\n",
      "Number of correct matches:  12592\n",
      "Validation accuracy: 20.987%\n",
      "Epoch 64/2000, Train Loss: 0.8885, Valid Loss: 0.8902\n",
      "Train loss: 419.2889778017998\n",
      "Number of correct matches:  12582\n",
      "Validation accuracy: 20.970%\n",
      "Epoch 65/2000, Train Loss: 0.8883, Valid Loss: 0.8910\n",
      "Train loss: 419.3760684132576\n",
      "Number of correct matches:  12553\n",
      "Validation accuracy: 20.922%\n",
      "Epoch 66/2000, Train Loss: 0.8885, Valid Loss: 0.8911\n",
      "Train loss: 419.731494307518\n",
      "Number of correct matches:  12676\n",
      "Validation accuracy: 21.127%\n",
      "Epoch 67/2000, Train Loss: 0.8893, Valid Loss: 0.8913\n",
      "Train loss: 419.46789968013763\n",
      "Number of correct matches:  12710\n",
      "Validation accuracy: 21.183%\n",
      "Epoch 68/2000, Train Loss: 0.8887, Valid Loss: 0.8914\n",
      "Train loss: 419.29808980226517\n",
      "Number of correct matches:  12581\n",
      "Validation accuracy: 20.968%\n",
      "Epoch 69/2000, Train Loss: 0.8883, Valid Loss: 0.8907\n",
      "Train loss: 419.53913551568985\n",
      "Number of correct matches:  12745\n",
      "Validation accuracy: 21.242%\n",
      "Epoch 70/2000, Train Loss: 0.8889, Valid Loss: 0.8913\n",
      "Train loss: 419.5342293381691\n",
      "Number of correct matches:  12602\n",
      "Validation accuracy: 21.003%\n",
      "Epoch 71/2000, Train Loss: 0.8888, Valid Loss: 0.8913\n",
      "Train loss: 419.579725921154\n",
      "Number of correct matches:  12635\n",
      "Validation accuracy: 21.058%\n",
      "Epoch 72/2000, Train Loss: 0.8889, Valid Loss: 0.8923\n",
      "Train loss: 419.3601067662239\n",
      "Number of correct matches:  12610\n",
      "Validation accuracy: 21.017%\n",
      "Epoch 73/2000, Train Loss: 0.8885, Valid Loss: 0.8913\n",
      "Train loss: 419.4302679300308\n",
      "Number of correct matches:  12598\n",
      "Validation accuracy: 20.997%\n",
      "Epoch 74/2000, Train Loss: 0.8886, Valid Loss: 0.8919\n",
      "Train loss: 419.34283769130707\n",
      "Number of correct matches:  12574\n",
      "Validation accuracy: 20.957%\n",
      "Epoch 75/2000, Train Loss: 0.8884, Valid Loss: 0.8904\n",
      "Train loss: 419.3890817165375\n",
      "Number of correct matches:  12627\n",
      "Validation accuracy: 21.045%\n",
      "Epoch 76/2000, Train Loss: 0.8885, Valid Loss: 0.8907\n",
      "Train loss: 419.4796318411827\n",
      "Number of correct matches:  12616\n",
      "Validation accuracy: 21.027%\n",
      "Epoch 77/2000, Train Loss: 0.8887, Valid Loss: 0.8888\n",
      "Model saved with validation loss: 0.8887565055135953\n",
      "\n",
      "Train loss: 419.3813968896866\n",
      "Number of correct matches:  12611\n",
      "Validation accuracy: 21.018%\n",
      "Epoch 78/2000, Train Loss: 0.8885, Valid Loss: 0.8925\n",
      "Train loss: 419.54500418901443\n",
      "Number of correct matches:  12555\n",
      "Validation accuracy: 20.925%\n",
      "Epoch 79/2000, Train Loss: 0.8889, Valid Loss: 0.8908\n",
      "Train loss: 419.3993465900421\n",
      "Number of correct matches:  12607\n",
      "Validation accuracy: 21.012%\n",
      "Epoch 80/2000, Train Loss: 0.8886, Valid Loss: 0.8893\n",
      "Train loss: 419.36549866199493\n",
      "Number of correct matches:  12657\n",
      "Validation accuracy: 21.095%\n",
      "Epoch 81/2000, Train Loss: 0.8885, Valid Loss: 0.8904\n",
      "Train loss: 419.5993613600731\n",
      "Number of correct matches:  12587\n",
      "Validation accuracy: 20.978%\n",
      "Epoch 82/2000, Train Loss: 0.8890, Valid Loss: 0.8915\n",
      "Train loss: 419.5960662961006\n",
      "Number of correct matches:  12654\n",
      "Validation accuracy: 21.090%\n",
      "Epoch 83/2000, Train Loss: 0.8890, Valid Loss: 0.8904\n",
      "Train loss: 419.71509140729904\n",
      "Number of correct matches:  12534\n",
      "Validation accuracy: 20.890%\n",
      "Epoch 84/2000, Train Loss: 0.8892, Valid Loss: 0.8906\n",
      "Train loss: 419.4497336745262\n",
      "Number of correct matches:  12559\n",
      "Validation accuracy: 20.932%\n",
      "Epoch 85/2000, Train Loss: 0.8887, Valid Loss: 0.8924\n",
      "Train loss: 419.47891414165497\n",
      "Number of correct matches:  12575\n",
      "Validation accuracy: 20.958%\n",
      "Epoch 86/2000, Train Loss: 0.8887, Valid Loss: 0.8913\n",
      "Train loss: 419.5540832877159\n",
      "Number of correct matches:  12545\n",
      "Validation accuracy: 20.908%\n",
      "Epoch 87/2000, Train Loss: 0.8889, Valid Loss: 0.8906\n",
      "Train loss: 419.4736883044243\n",
      "Number of correct matches:  12524\n",
      "Validation accuracy: 20.873%\n",
      "Epoch 88/2000, Train Loss: 0.8887, Valid Loss: 0.8910\n",
      "Train loss: 419.6339789032936\n",
      "Number of correct matches:  12664\n",
      "Validation accuracy: 21.107%\n",
      "Epoch 89/2000, Train Loss: 0.8891, Valid Loss: 0.8911\n",
      "Train loss: 419.4405189752579\n",
      "Number of correct matches:  12596\n",
      "Validation accuracy: 20.993%\n",
      "Epoch 90/2000, Train Loss: 0.8886, Valid Loss: 0.8914\n",
      "Train loss: 419.5375505685806\n",
      "Number of correct matches:  12633\n",
      "Validation accuracy: 21.055%\n",
      "Epoch 91/2000, Train Loss: 0.8889, Valid Loss: 0.8902\n",
      "Train loss: 419.43071579933167\n",
      "Number of correct matches:  12519\n",
      "Validation accuracy: 20.865%\n",
      "Epoch 92/2000, Train Loss: 0.8886, Valid Loss: 0.8901\n",
      "Train loss: 419.54563450813293\n",
      "Number of correct matches:  12606\n",
      "Validation accuracy: 21.010%\n",
      "Epoch 93/2000, Train Loss: 0.8889, Valid Loss: 0.8920\n",
      "Train loss: 419.51237362623215\n",
      "Number of correct matches:  12596\n",
      "Validation accuracy: 20.993%\n",
      "Epoch 94/2000, Train Loss: 0.8888, Valid Loss: 0.8912\n",
      "Train loss: 419.4269315600395\n",
      "Number of correct matches:  12594\n",
      "Validation accuracy: 20.990%\n",
      "Epoch 95/2000, Train Loss: 0.8886, Valid Loss: 0.8910\n",
      "Train loss: 419.4348706007004\n",
      "Number of correct matches:  12585\n",
      "Validation accuracy: 20.975%\n",
      "Epoch 96/2000, Train Loss: 0.8886, Valid Loss: 0.8916\n",
      "Train loss: 419.34347891807556\n",
      "Number of correct matches:  12666\n",
      "Validation accuracy: 21.110%\n",
      "Epoch 97/2000, Train Loss: 0.8884, Valid Loss: 0.8913\n",
      "Train loss: 419.48344898223877\n",
      "Number of correct matches:  12647\n",
      "Validation accuracy: 21.078%\n",
      "Epoch 98/2000, Train Loss: 0.8887, Valid Loss: 0.8905\n",
      "Train loss: 419.57389742136\n",
      "Number of correct matches:  12645\n",
      "Validation accuracy: 21.075%\n",
      "Epoch 99/2000, Train Loss: 0.8889, Valid Loss: 0.8912\n",
      "Train loss: 419.4586151242256\n",
      "Number of correct matches:  12596\n",
      "Validation accuracy: 20.993%\n",
      "Epoch 100/2000, Train Loss: 0.8887, Valid Loss: 0.8911\n",
      "Train loss: 419.60489958524704\n",
      "Number of correct matches:  12638\n",
      "Validation accuracy: 21.063%\n",
      "Epoch 101/2000, Train Loss: 0.8890, Valid Loss: 0.8896\n",
      "Train loss: 419.3955492377281\n",
      "Number of correct matches:  12627\n",
      "Validation accuracy: 21.045%\n",
      "Epoch 102/2000, Train Loss: 0.8885, Valid Loss: 0.8908\n",
      "Train loss: 419.46366715431213\n",
      "Number of correct matches:  12560\n",
      "Validation accuracy: 20.933%\n",
      "Epoch 103/2000, Train Loss: 0.8887, Valid Loss: 0.8903\n",
      "Train loss: 419.41949754953384\n",
      "Number of correct matches:  12601\n",
      "Validation accuracy: 21.002%\n",
      "Epoch 104/2000, Train Loss: 0.8886, Valid Loss: 0.8913\n",
      "Train loss: 419.39904594421387\n",
      "Number of correct matches:  12584\n",
      "Validation accuracy: 20.973%\n",
      "Epoch 105/2000, Train Loss: 0.8886, Valid Loss: 0.8921\n",
      "Train loss: 419.5906167626381\n",
      "Number of correct matches:  12628\n",
      "Validation accuracy: 21.047%\n",
      "Epoch 106/2000, Train Loss: 0.8890, Valid Loss: 0.8915\n",
      "Train loss: 419.4253726005554\n",
      "Number of correct matches:  12668\n",
      "Validation accuracy: 21.113%\n",
      "Epoch 107/2000, Train Loss: 0.8886, Valid Loss: 0.8916\n",
      "Train loss: 419.45848059654236\n",
      "Number of correct matches:  12595\n",
      "Validation accuracy: 20.992%\n",
      "Epoch 108/2000, Train Loss: 0.8887, Valid Loss: 0.8913\n",
      "Train loss: 419.46877986192703\n",
      "Number of correct matches:  12544\n",
      "Validation accuracy: 20.907%\n",
      "Epoch 109/2000, Train Loss: 0.8887, Valid Loss: 0.8915\n",
      "Train loss: 419.5527058839798\n",
      "Number of correct matches:  12616\n",
      "Validation accuracy: 21.027%\n",
      "Epoch 110/2000, Train Loss: 0.8889, Valid Loss: 0.8917\n",
      "Train loss: 419.52907252311707\n",
      "Number of correct matches:  12532\n",
      "Validation accuracy: 20.887%\n",
      "Epoch 111/2000, Train Loss: 0.8888, Valid Loss: 0.8914\n",
      "Train loss: 419.5166454911232\n",
      "Number of correct matches:  12690\n",
      "Validation accuracy: 21.150%\n",
      "Epoch 112/2000, Train Loss: 0.8888, Valid Loss: 0.8908\n",
      "Train loss: 419.4199471473694\n",
      "Number of correct matches:  12526\n",
      "Validation accuracy: 20.877%\n",
      "Epoch 113/2000, Train Loss: 0.8886, Valid Loss: 0.8918\n",
      "Train loss: 419.5804057121277\n",
      "Number of correct matches:  12578\n",
      "Validation accuracy: 20.963%\n",
      "Epoch 114/2000, Train Loss: 0.8889, Valid Loss: 0.8915\n",
      "Train loss: 419.3978148698807\n",
      "Number of correct matches:  12466\n",
      "Validation accuracy: 20.777%\n",
      "Epoch 115/2000, Train Loss: 0.8886, Valid Loss: 0.8902\n",
      "Train loss: 419.5735349059105\n",
      "Number of correct matches:  12620\n",
      "Validation accuracy: 21.033%\n",
      "Epoch 116/2000, Train Loss: 0.8889, Valid Loss: 0.8915\n",
      "Train loss: 419.652390897274\n",
      "Number of correct matches:  12667\n",
      "Validation accuracy: 21.112%\n",
      "Epoch 117/2000, Train Loss: 0.8891, Valid Loss: 0.8913\n",
      "Train loss: 419.47983080148697\n",
      "Number of correct matches:  12737\n",
      "Validation accuracy: 21.228%\n",
      "Epoch 118/2000, Train Loss: 0.8887, Valid Loss: 0.8915\n",
      "Train loss: 419.46876418590546\n",
      "Number of correct matches:  12707\n",
      "Validation accuracy: 21.178%\n",
      "Epoch 119/2000, Train Loss: 0.8887, Valid Loss: 0.8909\n",
      "Train loss: 419.48396039009094\n",
      "Number of correct matches:  12606\n",
      "Validation accuracy: 21.010%\n",
      "Epoch 120/2000, Train Loss: 0.8887, Valid Loss: 0.8914\n",
      "Train loss: 419.5169081687927\n",
      "Number of correct matches:  12590\n",
      "Validation accuracy: 20.983%\n",
      "Epoch 121/2000, Train Loss: 0.8888, Valid Loss: 0.8905\n",
      "Train loss: 419.3674714565277\n",
      "Number of correct matches:  12680\n",
      "Validation accuracy: 21.133%\n",
      "Epoch 122/2000, Train Loss: 0.8885, Valid Loss: 0.8916\n",
      "Train loss: 419.2807163596153\n",
      "Number of correct matches:  12529\n",
      "Validation accuracy: 20.882%\n",
      "Epoch 123/2000, Train Loss: 0.8883, Valid Loss: 0.8934\n",
      "Train loss: 419.61212372779846\n",
      "Number of correct matches:  12680\n",
      "Validation accuracy: 21.133%\n",
      "Epoch 124/2000, Train Loss: 0.8890, Valid Loss: 0.8909\n",
      "Train loss: 419.4704689383507\n",
      "Number of correct matches:  12657\n",
      "Validation accuracy: 21.095%\n",
      "Epoch 125/2000, Train Loss: 0.8887, Valid Loss: 0.8904\n",
      "Train loss: 419.68289786577225\n",
      "Number of correct matches:  12613\n",
      "Validation accuracy: 21.022%\n",
      "Epoch 126/2000, Train Loss: 0.8892, Valid Loss: 0.8909\n",
      "Train loss: 419.6450058221817\n",
      "Number of correct matches:  12686\n",
      "Validation accuracy: 21.143%\n",
      "Epoch 127/2000, Train Loss: 0.8891, Valid Loss: 0.8903\n",
      "Train loss: 419.11276203393936\n",
      "Number of correct matches:  12632\n",
      "Validation accuracy: 21.053%\n",
      "Epoch 128/2000, Train Loss: 0.8880, Valid Loss: 0.8911\n",
      "Train loss: 419.4387283921242\n",
      "Number of correct matches:  12617\n",
      "Validation accuracy: 21.028%\n",
      "Epoch 129/2000, Train Loss: 0.8886, Valid Loss: 0.8904\n",
      "Train loss: 419.66468608379364\n",
      "Number of correct matches:  12575\n",
      "Validation accuracy: 20.958%\n",
      "Epoch 130/2000, Train Loss: 0.8891, Valid Loss: 0.8908\n",
      "Train loss: 419.3511632680893\n",
      "Number of correct matches:  12596\n",
      "Validation accuracy: 20.993%\n",
      "Epoch 131/2000, Train Loss: 0.8885, Valid Loss: 0.8915\n",
      "Train loss: 419.5170255303383\n",
      "Number of correct matches:  12656\n",
      "Validation accuracy: 21.093%\n",
      "Epoch 132/2000, Train Loss: 0.8888, Valid Loss: 0.8913\n",
      "Train loss: 419.5122966170311\n",
      "Number of correct matches:  12539\n",
      "Validation accuracy: 20.898%\n",
      "Epoch 133/2000, Train Loss: 0.8888, Valid Loss: 0.8902\n",
      "Train loss: 419.28979194164276\n",
      "Number of correct matches:  12606\n",
      "Validation accuracy: 21.010%\n",
      "Epoch 134/2000, Train Loss: 0.8883, Valid Loss: 0.8911\n",
      "Train loss: 419.5518540740013\n",
      "Number of correct matches:  12644\n",
      "Validation accuracy: 21.073%\n",
      "Epoch 135/2000, Train Loss: 0.8889, Valid Loss: 0.8915\n",
      "Train loss: 419.2056288719177\n",
      "Number of correct matches:  12676\n",
      "Validation accuracy: 21.127%\n",
      "Epoch 136/2000, Train Loss: 0.8881, Valid Loss: 0.8905\n",
      "Train loss: 419.6271874308586\n",
      "Number of correct matches:  12761\n",
      "Validation accuracy: 21.268%\n",
      "Epoch 137/2000, Train Loss: 0.8890, Valid Loss: 0.8918\n",
      "Train loss: 419.60337221622467\n",
      "Number of correct matches:  12592\n",
      "Validation accuracy: 20.987%\n",
      "Epoch 138/2000, Train Loss: 0.8890, Valid Loss: 0.8908\n",
      "Train loss: 419.3849183320999\n",
      "Number of correct matches:  12603\n",
      "Validation accuracy: 21.005%\n",
      "Epoch 139/2000, Train Loss: 0.8885, Valid Loss: 0.8910\n",
      "Train loss: 419.5941423177719\n",
      "Number of correct matches:  12545\n",
      "Validation accuracy: 20.908%\n",
      "Epoch 140/2000, Train Loss: 0.8890, Valid Loss: 0.8906\n",
      "Train loss: 419.45426458120346\n",
      "Number of correct matches:  12641\n",
      "Validation accuracy: 21.068%\n",
      "Epoch 141/2000, Train Loss: 0.8887, Valid Loss: 0.8916\n",
      "Train loss: 419.353190779686\n",
      "Number of correct matches:  12609\n",
      "Validation accuracy: 21.015%\n",
      "Epoch 142/2000, Train Loss: 0.8885, Valid Loss: 0.8910\n",
      "Train loss: 419.44679802656174\n",
      "Number of correct matches:  12633\n",
      "Validation accuracy: 21.055%\n",
      "Epoch 143/2000, Train Loss: 0.8887, Valid Loss: 0.8905\n",
      "Train loss: 419.3715626001358\n",
      "Number of correct matches:  12543\n",
      "Validation accuracy: 20.905%\n",
      "Epoch 144/2000, Train Loss: 0.8885, Valid Loss: 0.8906\n",
      "Train loss: 419.32223653793335\n",
      "Number of correct matches:  12668\n",
      "Validation accuracy: 21.113%\n",
      "Epoch 145/2000, Train Loss: 0.8884, Valid Loss: 0.8903\n",
      "Train loss: 419.5531556606293\n",
      "Number of correct matches:  12657\n",
      "Validation accuracy: 21.095%\n",
      "Epoch 146/2000, Train Loss: 0.8889, Valid Loss: 0.8906\n",
      "Train loss: 419.6681098937988\n",
      "Number of correct matches:  12537\n",
      "Validation accuracy: 20.895%\n",
      "Epoch 147/2000, Train Loss: 0.8891, Valid Loss: 0.8897\n",
      "Train loss: 419.37749868631363\n",
      "Number of correct matches:  12580\n",
      "Validation accuracy: 20.967%\n",
      "Epoch 148/2000, Train Loss: 0.8885, Valid Loss: 0.8906\n",
      "Train loss: 419.51947152614594\n",
      "Number of correct matches:  12633\n",
      "Validation accuracy: 21.055%\n",
      "Epoch 149/2000, Train Loss: 0.8888, Valid Loss: 0.8915\n",
      "Train loss: 419.3321690559387\n",
      "Number of correct matches:  12516\n",
      "Validation accuracy: 20.860%\n",
      "Epoch 150/2000, Train Loss: 0.8884, Valid Loss: 0.8911\n",
      "Train loss: 419.5476233959198\n",
      "Number of correct matches:  12537\n",
      "Validation accuracy: 20.895%\n",
      "Epoch 151/2000, Train Loss: 0.8889, Valid Loss: 0.8915\n",
      "Train loss: 419.4889124035835\n",
      "Number of correct matches:  12632\n",
      "Validation accuracy: 21.053%\n",
      "Epoch 152/2000, Train Loss: 0.8887, Valid Loss: 0.8912\n",
      "Train loss: 419.557035446167\n",
      "Number of correct matches:  12728\n",
      "Validation accuracy: 21.213%\n",
      "Epoch 153/2000, Train Loss: 0.8889, Valid Loss: 0.8913\n",
      "Train loss: 419.3430326581001\n",
      "Number of correct matches:  12703\n",
      "Validation accuracy: 21.172%\n",
      "Epoch 154/2000, Train Loss: 0.8884, Valid Loss: 0.8903\n",
      "Train loss: 419.4599023461342\n",
      "Number of correct matches:  12648\n",
      "Validation accuracy: 21.080%\n",
      "Epoch 155/2000, Train Loss: 0.8887, Valid Loss: 0.8903\n",
      "Train loss: 419.50467920303345\n",
      "Number of correct matches:  12630\n",
      "Validation accuracy: 21.050%\n",
      "Epoch 156/2000, Train Loss: 0.8888, Valid Loss: 0.8914\n",
      "Train loss: 419.2647696733475\n",
      "Number of correct matches:  12606\n",
      "Validation accuracy: 21.010%\n",
      "Epoch 157/2000, Train Loss: 0.8883, Valid Loss: 0.8906\n",
      "Train loss: 419.470050573349\n",
      "Number of correct matches:  12542\n",
      "Validation accuracy: 20.903%\n",
      "Epoch 158/2000, Train Loss: 0.8887, Valid Loss: 0.8906\n",
      "Train loss: 419.3836323618889\n",
      "Number of correct matches:  12519\n",
      "Validation accuracy: 20.865%\n",
      "Epoch 159/2000, Train Loss: 0.8885, Valid Loss: 0.8918\n",
      "Train loss: 419.6188833117485\n",
      "Number of correct matches:  12627\n",
      "Validation accuracy: 21.045%\n",
      "Epoch 160/2000, Train Loss: 0.8890, Valid Loss: 0.8914\n",
      "Train loss: 419.3956023454666\n",
      "Number of correct matches:  12626\n",
      "Validation accuracy: 21.043%\n",
      "Epoch 161/2000, Train Loss: 0.8886, Valid Loss: 0.8911\n",
      "Train loss: 419.5297429561615\n",
      "Number of correct matches:  12649\n",
      "Validation accuracy: 21.082%\n",
      "Epoch 162/2000, Train Loss: 0.8888, Valid Loss: 0.8912\n",
      "Train loss: 419.52479845285416\n",
      "Number of correct matches:  12603\n",
      "Validation accuracy: 21.005%\n",
      "Epoch 163/2000, Train Loss: 0.8888, Valid Loss: 0.8906\n",
      "Train loss: 419.51610893011093\n",
      "Number of correct matches:  12631\n",
      "Validation accuracy: 21.052%\n",
      "Epoch 164/2000, Train Loss: 0.8888, Valid Loss: 0.8906\n",
      "Train loss: 419.2893802523613\n",
      "Number of correct matches:  12611\n",
      "Validation accuracy: 21.018%\n",
      "Epoch 165/2000, Train Loss: 0.8883, Valid Loss: 0.8908\n",
      "Train loss: 419.47974503040314\n",
      "Number of correct matches:  12707\n",
      "Validation accuracy: 21.178%\n",
      "Epoch 166/2000, Train Loss: 0.8887, Valid Loss: 0.8907\n",
      "Train loss: 419.42573350667953\n",
      "Number of correct matches:  12661\n",
      "Validation accuracy: 21.102%\n",
      "Epoch 167/2000, Train Loss: 0.8886, Valid Loss: 0.8915\n",
      "Train loss: 419.4201907515526\n",
      "Number of correct matches:  12676\n",
      "Validation accuracy: 21.127%\n",
      "Epoch 168/2000, Train Loss: 0.8886, Valid Loss: 0.8892\n",
      "Train loss: 419.72840732336044\n",
      "Number of correct matches:  12627\n",
      "Validation accuracy: 21.045%\n",
      "Epoch 169/2000, Train Loss: 0.8893, Valid Loss: 0.8902\n",
      "Train loss: 419.6680317521095\n",
      "Number of correct matches:  12558\n",
      "Validation accuracy: 20.930%\n",
      "Epoch 170/2000, Train Loss: 0.8891, Valid Loss: 0.8910\n",
      "Train loss: 419.4802578687668\n",
      "Number of correct matches:  12576\n",
      "Validation accuracy: 20.960%\n",
      "Epoch 171/2000, Train Loss: 0.8887, Valid Loss: 0.8906\n",
      "Train loss: 419.24708729982376\n",
      "Number of correct matches:  12584\n",
      "Validation accuracy: 20.973%\n",
      "Epoch 172/2000, Train Loss: 0.8882, Valid Loss: 0.8929\n",
      "Train loss: 419.68239402770996\n",
      "Number of correct matches:  12638\n",
      "Validation accuracy: 21.063%\n",
      "Epoch 173/2000, Train Loss: 0.8892, Valid Loss: 0.8916\n",
      "Train loss: 419.54619777202606\n",
      "Number of correct matches:  12685\n",
      "Validation accuracy: 21.142%\n",
      "Epoch 174/2000, Train Loss: 0.8889, Valid Loss: 0.8905\n",
      "Train loss: 419.5038355588913\n",
      "Number of correct matches:  12689\n",
      "Validation accuracy: 21.148%\n",
      "Epoch 175/2000, Train Loss: 0.8888, Valid Loss: 0.8905\n",
      "Train loss: 419.7293120622635\n",
      "Number of correct matches:  12580\n",
      "Validation accuracy: 20.967%\n",
      "Epoch 176/2000, Train Loss: 0.8893, Valid Loss: 0.8902\n",
      "Train loss: 419.2860106229782\n",
      "Number of correct matches:  12621\n",
      "Validation accuracy: 21.035%\n",
      "Epoch 177/2000, Train Loss: 0.8883, Valid Loss: 0.8915\n",
      "Train loss: 419.6458153128624\n",
      "Number of correct matches:  12697\n",
      "Validation accuracy: 21.162%\n",
      "Epoch 178/2000, Train Loss: 0.8891, Valid Loss: 0.8921\n",
      "Train loss: 419.31697672605515\n",
      "Number of correct matches:  12621\n",
      "Validation accuracy: 21.035%\n",
      "Epoch 179/2000, Train Loss: 0.8884, Valid Loss: 0.8911\n",
      "Train loss: 419.43184918165207\n",
      "Number of correct matches:  12602\n",
      "Validation accuracy: 21.003%\n",
      "Epoch 180/2000, Train Loss: 0.8886, Valid Loss: 0.8901\n",
      "Train loss: 419.7470591664314\n",
      "Number of correct matches:  12626\n",
      "Validation accuracy: 21.043%\n",
      "Epoch 181/2000, Train Loss: 0.8893, Valid Loss: 0.8912\n",
      "Train loss: 419.4281550049782\n",
      "Number of correct matches:  12622\n",
      "Validation accuracy: 21.037%\n",
      "Epoch 182/2000, Train Loss: 0.8886, Valid Loss: 0.8906\n",
      "Train loss: 419.53494185209274\n",
      "Number of correct matches:  12628\n",
      "Validation accuracy: 21.047%\n",
      "Epoch 183/2000, Train Loss: 0.8888, Valid Loss: 0.8917\n",
      "Train loss: 419.4099658727646\n",
      "Number of correct matches:  12517\n",
      "Validation accuracy: 20.862%\n",
      "Epoch 184/2000, Train Loss: 0.8886, Valid Loss: 0.8920\n",
      "Train loss: 419.5832721590996\n",
      "Number of correct matches:  12630\n",
      "Validation accuracy: 21.050%\n",
      "Epoch 185/2000, Train Loss: 0.8889, Valid Loss: 0.8899\n",
      "Train loss: 419.4487866163254\n",
      "Number of correct matches:  12616\n",
      "Validation accuracy: 21.027%\n",
      "Epoch 186/2000, Train Loss: 0.8887, Valid Loss: 0.8909\n",
      "Train loss: 419.54089736938477\n",
      "Number of correct matches:  12578\n",
      "Validation accuracy: 20.963%\n",
      "Epoch 187/2000, Train Loss: 0.8889, Valid Loss: 0.8914\n",
      "Train loss: 419.5134246945381\n",
      "Number of correct matches:  12570\n",
      "Validation accuracy: 20.950%\n",
      "Epoch 188/2000, Train Loss: 0.8888, Valid Loss: 0.8909\n",
      "Train loss: 419.26271069049835\n",
      "Number of correct matches:  12638\n",
      "Validation accuracy: 21.063%\n",
      "Epoch 189/2000, Train Loss: 0.8883, Valid Loss: 0.8911\n",
      "Train loss: 419.5670139193535\n",
      "Number of correct matches:  12518\n",
      "Validation accuracy: 20.863%\n",
      "Epoch 190/2000, Train Loss: 0.8889, Valid Loss: 0.8908\n",
      "Train loss: 419.67720288038254\n",
      "Number of correct matches:  12600\n",
      "Validation accuracy: 21.000%\n",
      "Epoch 191/2000, Train Loss: 0.8891, Valid Loss: 0.8921\n",
      "Train loss: 419.43594777584076\n",
      "Number of correct matches:  12595\n",
      "Validation accuracy: 20.992%\n",
      "Epoch 192/2000, Train Loss: 0.8886, Valid Loss: 0.8901\n",
      "Train loss: 419.4059430360794\n",
      "Number of correct matches:  12674\n",
      "Validation accuracy: 21.123%\n",
      "Epoch 193/2000, Train Loss: 0.8886, Valid Loss: 0.8912\n",
      "Train loss: 419.540946662426\n",
      "Number of correct matches:  12732\n",
      "Validation accuracy: 21.220%\n",
      "Epoch 194/2000, Train Loss: 0.8889, Valid Loss: 0.8918\n",
      "Train loss: 419.68222665786743\n",
      "Number of correct matches:  12638\n",
      "Validation accuracy: 21.063%\n",
      "Epoch 195/2000, Train Loss: 0.8892, Valid Loss: 0.8904\n",
      "Train loss: 419.50935220718384\n",
      "Number of correct matches:  12678\n",
      "Validation accuracy: 21.130%\n",
      "Epoch 196/2000, Train Loss: 0.8888, Valid Loss: 0.8917\n",
      "Train loss: 419.50869458913803\n",
      "Number of correct matches:  12622\n",
      "Validation accuracy: 21.037%\n",
      "Epoch 197/2000, Train Loss: 0.8888, Valid Loss: 0.8908\n",
      "Train loss: 419.42097145318985\n",
      "Number of correct matches:  12585\n",
      "Validation accuracy: 20.975%\n",
      "Epoch 198/2000, Train Loss: 0.8886, Valid Loss: 0.8905\n",
      "Train loss: 419.5464627146721\n",
      "Number of correct matches:  12653\n",
      "Validation accuracy: 21.088%\n",
      "Epoch 199/2000, Train Loss: 0.8889, Valid Loss: 0.8907\n",
      "Train loss: 419.5447210073471\n",
      "Number of correct matches:  12561\n",
      "Validation accuracy: 20.935%\n",
      "Epoch 200/2000, Train Loss: 0.8889, Valid Loss: 0.8911\n",
      "Train loss: 419.42443186044693\n",
      "Number of correct matches:  12677\n",
      "Validation accuracy: 21.128%\n",
      "Epoch 201/2000, Train Loss: 0.8886, Valid Loss: 0.8914\n",
      "Train loss: 419.2730887532234\n",
      "Number of correct matches:  12607\n",
      "Validation accuracy: 21.012%\n",
      "Epoch 202/2000, Train Loss: 0.8883, Valid Loss: 0.8911\n",
      "Train loss: 419.5104189515114\n",
      "Number of correct matches:  12603\n",
      "Validation accuracy: 21.005%\n",
      "Epoch 203/2000, Train Loss: 0.8888, Valid Loss: 0.8905\n",
      "Train loss: 419.4749835729599\n",
      "Number of correct matches:  12463\n",
      "Validation accuracy: 20.772%\n",
      "Epoch 204/2000, Train Loss: 0.8887, Valid Loss: 0.8910\n",
      "Train loss: 419.5930635333061\n",
      "Number of correct matches:  12614\n",
      "Validation accuracy: 21.023%\n",
      "Epoch 205/2000, Train Loss: 0.8890, Valid Loss: 0.8904\n",
      "Train loss: 419.4155972599983\n",
      "Number of correct matches:  12677\n",
      "Validation accuracy: 21.128%\n",
      "Epoch 206/2000, Train Loss: 0.8886, Valid Loss: 0.8912\n",
      "Train loss: 419.4141277074814\n",
      "Number of correct matches:  12578\n",
      "Validation accuracy: 20.963%\n",
      "Epoch 207/2000, Train Loss: 0.8886, Valid Loss: 0.8910\n",
      "Train loss: 419.66605389118195\n",
      "Number of correct matches:  12588\n",
      "Validation accuracy: 20.980%\n",
      "Epoch 208/2000, Train Loss: 0.8891, Valid Loss: 0.8914\n",
      "Train loss: 419.5487108826637\n",
      "Number of correct matches:  12655\n",
      "Validation accuracy: 21.092%\n",
      "Epoch 209/2000, Train Loss: 0.8889, Valid Loss: 0.8925\n",
      "Train loss: 419.49672186374664\n",
      "Number of correct matches:  12616\n",
      "Validation accuracy: 21.027%\n",
      "Epoch 210/2000, Train Loss: 0.8888, Valid Loss: 0.8907\n",
      "Train loss: 419.2081546783447\n",
      "Number of correct matches:  12631\n",
      "Validation accuracy: 21.052%\n",
      "Epoch 211/2000, Train Loss: 0.8882, Valid Loss: 0.8909\n",
      "Train loss: 419.5712317228317\n",
      "Number of correct matches:  12625\n",
      "Validation accuracy: 21.042%\n",
      "Epoch 212/2000, Train Loss: 0.8889, Valid Loss: 0.8924\n",
      "Train loss: 419.66117227077484\n",
      "Number of correct matches:  12607\n",
      "Validation accuracy: 21.012%\n",
      "Epoch 213/2000, Train Loss: 0.8891, Valid Loss: 0.8921\n",
      "Train loss: 419.2152326107025\n",
      "Number of correct matches:  12625\n",
      "Validation accuracy: 21.042%\n",
      "Epoch 214/2000, Train Loss: 0.8882, Valid Loss: 0.8906\n",
      "Train loss: 419.49345231056213\n",
      "Number of correct matches:  12510\n",
      "Validation accuracy: 20.850%\n",
      "Epoch 215/2000, Train Loss: 0.8888, Valid Loss: 0.8906\n",
      "Train loss: 419.2928006052971\n",
      "Number of correct matches:  12540\n",
      "Validation accuracy: 20.900%\n",
      "Epoch 216/2000, Train Loss: 0.8883, Valid Loss: 0.8914\n",
      "Train loss: 419.4975079894066\n",
      "Number of correct matches:  12621\n",
      "Validation accuracy: 21.035%\n",
      "Epoch 217/2000, Train Loss: 0.8888, Valid Loss: 0.8907\n",
      "Train loss: 419.4527899622917\n",
      "Number of correct matches:  12701\n",
      "Validation accuracy: 21.168%\n",
      "Epoch 218/2000, Train Loss: 0.8887, Valid Loss: 0.8888\n",
      "Train loss: 419.6368052959442\n",
      "Number of correct matches:  12652\n",
      "Validation accuracy: 21.087%\n",
      "Epoch 219/2000, Train Loss: 0.8891, Valid Loss: 0.8912\n",
      "Train loss: 419.4890300631523\n",
      "Number of correct matches:  12648\n",
      "Validation accuracy: 21.080%\n",
      "Epoch 220/2000, Train Loss: 0.8887, Valid Loss: 0.8907\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/resnet\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mRESNET_VERSION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, train_loader, valid_loader, epochs, save_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m input_1 \u001b[38;5;241m=\u001b[39m input_1\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     26\u001b[0m input_2 \u001b[38;5;241m=\u001b[39m input_2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 28\u001b[0m emb_1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m emb_2 \u001b[38;5;241m=\u001b[39m model(input_2)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Step 1: Calculate max and min\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# max_1 = torch.max(emb_1)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# min_1 = torch.min(emb_1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# emb_1 = (emb_1 - min_1) / (max_1 - min_1)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# emb_2 = (emb_2 - min_2) / (max_2 - min_2)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torchvision\\models\\resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[0;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torchvision\\models\\resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[0;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\miniconda3\\envs\\trust-me\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, criterion, optimizer, train_dl, eval_dl, n_epochs, f\"models/resnet{RESNET_VERSION}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1ca7389cf50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(loss_history, label=\"loss\")\n",
    "plt.plot(accuracy_history, label=\"accuracy\") \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch_cutoff = np.argmin(loss_history)\n",
    "epoch_cutoff = 20\n",
    "\n",
    "# Create a figure and primary y-axis\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot data on the primary y-axis\n",
    "ax1.plot(loss_history[:epoch_cutoff], color='blue', label='Training loss')\n",
    "ax1.set_ylabel('Training loss', color='blue')\n",
    "ax1.set_ylim(0, max(loss_history) + 50)\n",
    "\n",
    "# Create a secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "# Plot data on the secondary y-axis\n",
    "ax2.plot(accuracy_history[:epoch_cutoff], color='red', label='Validation accuracy')\n",
    "ax2.set_ylabel('Validation accuracy [%]', color='red')\n",
    "\n",
    "# Add legend\n",
    "ax1.legend(loc='best')\n",
    "ax2.legend(loc='best')\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_xticks([2 * i for i in range(epoch_cutoff // 2)])\n",
    "# Show plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
